{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd75652b-a863-4550-916d-09581782671c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import time\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import onnx\n",
    "import onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "311f066c-8bff-45de-b333-8725abdcef6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#checking the device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cfb3dd9-a852-4084-8de3-56d7114446f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models are: ['bmshj2018-factorized', 'bmshj2018-factorized-relu', 'bmshj2018-hyperprior', 'mbt2018-mean', 'mbt2018', 'cheng2020-anchor', 'cheng2020-attn', 'bmshj2018-hyperprior-vbr', 'mbt2018-mean-vbr', 'mbt2018-vbr', 'hrtzxf2022-pcc-rec', 'sfu2023-pcc-rec-pointnet', 'sfu2024-pcc-rec-pointnet2-ssg', 'ssf2020']\n",
      "Model encoder architecture bmshj2018-factorized\n",
      "Encoder Parameters: 2998147\n"
     ]
    }
   ],
   "source": [
    "#importing the models from the compressai zoo\n",
    "from compressai.zoo import models\n",
    "#models avaialble are \n",
    "model_names=models.keys()\n",
    "print(\"models are:\",list(model_names))\n",
    "\n",
    "quality=4\n",
    "metric=\"ms-ssim\" #\"mse\"\n",
    "model_name=\"bmshj2018-factorized\"\n",
    "#model_name=\"mbt2018-mean\"\n",
    "#mode_name=\"mbt2018-mean\"\n",
    "\n",
    "net = models[model_name](quality=quality,metric=metric, pretrained=True).eval().to(device)\n",
    "print(\"Model encoder architecture\",model_name)\n",
    "\n",
    "print(f'Encoder Parameters: {sum(p.numel() for p in net.parameters())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b1ed2c0-b0c0-45bb-9a83-7fbdc3a512de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: bmshj2018-factorized quality: 4 metric: mse param: 2998147\n",
      "\n",
      "Now exporting the model\n",
      "model is exported\n",
      "model_name: bmshj2018-factorized quality: 4 metric: ms-ssim param: 2998147\n",
      "\n",
      "Now exporting the model\n",
      "model is exported\n",
      "model_name: bmshj2018-factorized quality: 6 metric: mse param: 7030531\n",
      "\n",
      "Now exporting the model\n",
      "model is exported\n",
      "model_name: bmshj2018-factorized quality: 6 metric: ms-ssim param: 7030531\n",
      "\n",
      "Now exporting the model\n",
      "model is exported\n",
      "model_name: bmshj2018-factorized quality: 8 metric: mse param: 7030531\n",
      "\n",
      "Now exporting the model\n",
      "model is exported\n",
      "model_name: bmshj2018-factorized quality: 8 metric: ms-ssim param: 7030531\n",
      "\n",
      "Now exporting the model\n",
      "model is exported\n"
     ]
    }
   ],
   "source": [
    "#trying indivisually for each model configuration\n",
    "dl_models={} \n",
    "\n",
    "model_name=\"bmshj2018-factorized\"\n",
    "\n",
    "export_path=r\"C:\\Swapnil\\Narrowband_DRONE\\Image_compression_code\\Final_DL_compession_September_24\\PyTorch_inference\\pytorch_models\"\n",
    "\n",
    "for quality in [4,6,8]:\n",
    "    for metric in [\"mse\",\"ms-ssim\"]:\n",
    "        net=models[model_name](quality=quality,metric=metric,pretrained=True).eval().to(device)\n",
    "        parameters=sum(p.numel() for p in net.parameters())\n",
    "        model_name_final=model_name+\"_\"+str(quality)+\"_\"+metric\n",
    "        dl_models[model_name_final]=[net,[model_name,quality,metric,parameters]]\n",
    "        print(\"model_name:\",model_name,\"quality:\",quality,\"metric:\",metric,\"param:\",parameters)\n",
    "        model_export_name=model_name_final\n",
    "        print()\n",
    "        print(\"Now exporting the model\")\n",
    "        model_export_path=os.path.join(export_path,model_export_name)\n",
    "\n",
    "        #input for the model \n",
    "\n",
    "        \n",
    "        torch.save(net,model_export_path)\n",
    "        print(\"model is exported\")\n",
    "        time.sleep(1)    \n",
    "            \n",
    "        #except:\n",
    "        #    print(\"FAILED model_name:\",model_name,\"quality:\",quality,\"metric:\",metric,\"param:\",parameters)\n",
    "        #time.sleep(5)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e42b9a8-04c0-49c0-8aca-54ebeca29a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compress ai models dictionary\n",
    "\n",
    "#dl_models={} #contains the model name and the parameter and the quality \n",
    "#for model_name in model_names:\n",
    "#    for quality in [4,6,8]:\n",
    "#        for metric in [\"mse\",\"ms-ssim\"]:\n",
    "#            try:\n",
    "#                net=models[model_name](quality=quality,metric=metric,pretrained=True).eval().to(device)\n",
    "#                parameters=sum(p.numel() for p in net.parameters())\n",
    "#                model_name_final=mode_name+\"_\"+str(quality)+\"_\"+metric\n",
    "#                dl_models[model_name_final]=[net,[model_name,quality,metric,parameters]]\n",
    "#                print(\"model_name:\",model_name,\"quality:\",quality,\"metric:\",metric,\"param:\",parameters)\n",
    "#            except:\n",
    "#                print(\"FAILED model_name:\",model_name,\"quality:\",quality,\"metric:\",metric,\"param:\",parameters)\n",
    "#            time.sleep(5)\n",
    "\n",
    "#print(\"models we received are:\",dl_models.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1799145c-3ada-4f4b-acb0-8ce0f1b9b5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing the function to export the models to the disc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "458e89ba-ac71-4f0c-8f3a-236bfe015060",
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing the functions to export the models to the ONNX versions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "13234bd5-5929-4536-9436-bfdcdd528c1d",
   "metadata": {},
   "source": [
    "Write a function to make a dictionary for all the models with the name, quality ,metric and the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8d7307c9-70fb-41d9-a770-92a99149597d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export directory created\n",
      "dataset has: 50 images\n",
      "Using the image shape as: (1280, 1280)\n"
     ]
    }
   ],
   "source": [
    "#Dataset handling\n",
    "dataset_path=r\"C:\\Swapnil\\Narrowband_DRONE\\Image_compression_code\\Final_DL_compession_September_24\\Dataset_50\"\n",
    "export_dataset_path=r\"C:\\Swapnil\\Narrowband_DRONE\\Image_compression_code\\Final_DL_compession_September_24\\PyTorch_inference\\CompressAI Models\\modified_dataset\"\n",
    "\n",
    "\n",
    "def image_handling(dataset_path):\n",
    "    lst=os.listdir(dataset_path)\n",
    "    images=[]\n",
    "    for image in lst:\n",
    "        if image.lower().endswith(\"jpg\") or image.lower().endswith(\"jpeg\") or image.lower().endswith(\"png\") or image.lower().endswith(\"webp\"):\n",
    "            images.append(image)\n",
    "    print(\"dataset has:\",len(images),\"images\")\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "def preprocess_gray(image_path,image_shape):\n",
    "    \"\"\"\n",
    "        image_shape is expected to be a tuple\n",
    "        \n",
    "        This function resizes the image and converts\n",
    "        it to the grayscale and returns in the form of \n",
    "        numpy array\n",
    "        \"\"\"\n",
    "    image=Image.open(image_path).convert(\"L\")\n",
    "    #resize the image\n",
    "    image=image.resize(image_shape)\n",
    "    \n",
    "    #convert to numpy and normalize\n",
    "    image=np.array(image)/255.0\n",
    "\n",
    "    #print(\"image shape:\",image.shape)\n",
    "    \n",
    "    #create an empty numpy array\n",
    "    temp=np.zeros(image_shape)\n",
    "    temp=np.expand_dims(temp,axis=0)\n",
    "\n",
    "    #now make the array of size 3,shape\n",
    "    img_blank=np.repeat(temp,3,axis=0)\n",
    "    #img_blank=np.zeros((3,1232,1640))\n",
    "\n",
    "    #assign the gray image to the blank array\n",
    "    img_blank[0]=image\n",
    "    \n",
    "    img_final=img_blank.astype(np.float32)\n",
    "    #img_final=np.expand_dims(img_blank,axis=0).astype(np.float32)\n",
    "    #print(\"shape of the final image:\",img_final.shape,\"type:\",type(img_final))\n",
    "   \n",
    "    return img_final\n",
    "\n",
    "\n",
    "def save_compressed_image(output, save_path):\n",
    "    \"\"\"\n",
    "    Save the compressed output image from the model inference.\n",
    "    \"\"\"\n",
    "    # Post-process the output (if needed)\n",
    "    output = output.squeeze(0)  # Remove batch dimension\n",
    "    output=output[0]\n",
    "    print(\"dimension\",output.shape)\n",
    "    \n",
    "    # Convert to HWC format (Height x Width x Channels)\n",
    "    #output = np.transpose(output, (1, 2, 0))\n",
    "    \n",
    "    # Clip values to valid range (0, 1) and convert to 8-bit (0-255)\n",
    "    output = np.clip(output, 0, 1) * 255.0\n",
    "    output = output.astype(np.uint8)\n",
    "    \n",
    "    # Convert to image format and save\n",
    "    output_image = Image.fromarray(output,mode=\"L\")\n",
    "    print(np.array(output_image).shape)\n",
    "    #output_image=output_image.convert(\"L\")\n",
    "    output_image.save(save_path,format=\"JPEG\",quality=90) \n",
    "\n",
    "\n",
    "def export_modified_dataset(dataset_path,export_dataset_path,image_shape,gray):\n",
    "\n",
    "    \"\"\"\n",
    "        image_shape is the shape of the expected images\n",
    "    the original images will be resized to this shape\n",
    "\n",
    "    gray is boolean, if True the dataset will be turned into gray\n",
    "    else not\n",
    "    \n",
    "    \"\"\"\n",
    "    if gray==True:\n",
    "        export_dataset_path=os.path.join(export_dataset_path,\"gray\")\n",
    "    else:\n",
    "        export_dataset_path=os.path.join(export_dataset_path,\"color\")\n",
    "        \n",
    "    if os.path.exists(export_dataset_path):\n",
    "        print(\"export directory already exists\")\n",
    "    else:\n",
    "        #make the path\n",
    "        os.makedirs(export_dataset_path)\n",
    "        print(\"export directory created\")\n",
    "\n",
    "    #getting the dataset\n",
    "    images=image_handling(dataset_path)\n",
    "\n",
    "    print(\"Using the image shape as:\",image_shape)\n",
    "\n",
    "    \n",
    "    if gray==True:\n",
    "        for image in images:\n",
    "            image_path=os.path.join(dataset_path,image)\n",
    "            img=preprocess_gray(image_path,image_shape)\n",
    "            save_path=os.path.join(export_dataset_path,image.split('.')[0]+\"_gray.png\")#exporting the dataset as the PNG\n",
    "            img=img[0]\n",
    "            #img=img.transpose()\n",
    "            #print(\"shape:\",img.shape,\"type :\",type(img))\n",
    "            img=np.clip(img,0,1)*255.0\n",
    "            img=img.astype(np.uint8)\n",
    "            img=Image.fromarray(img,mode=\"L\")\n",
    "            img.save(save_path,format=\"PNG\") #quality not defined using the highest by default\n",
    "    else:\n",
    "        #we just resize the images based on the final size\n",
    "        for image in images:\n",
    "            image_path=os.path.join(dataset_path,image)\n",
    "            #open the image and resize the image\n",
    "            img=Image.open(image_path)\n",
    "            #resize the image\n",
    "            img=img.resize(image_shape)\n",
    "            #convert to numpy and normalize\n",
    "            img=np.array(img)/255.0\n",
    "            img=np.clip(img,0,1)*255.0\n",
    "            img=img.astype(np.uint8)\n",
    "            save_path=os.path.join(export_dataset_path,image.split('.')[0]+\"_resize.png\")#exporting the dataset as the PNG\n",
    "            img=Image.fromarray(img)\n",
    "            img.save(save_path,format=\"PNG\") #quality not defined using the highest by default\n",
    "            \n",
    "    print(\"The dataset modification is completed\")\n",
    "    return export_dataset_path\n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "image_shape=(1280,1280)\n",
    "gray=False\n",
    "\n",
    "#final dataset path is the function output which will be used finally based on the gray option\n",
    "export_dataset_path=export_modified_dataset(dataset_path,export_dataset_path,image_shape,gray)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79350672-5eee-438b-a776-8002eaae19a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get write the functions to get the quality of the output from the compression in terms of PSNR SSIM and LPIPS\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
