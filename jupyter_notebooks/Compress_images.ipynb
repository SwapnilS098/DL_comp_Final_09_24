{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1381d4f4-cb13-4c0f-a53c-b54112278eb6",
   "metadata": {},
   "source": [
    "Program to use BMSHJ for compressing the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9437b305-38e7-4e9c-a604-c47477a728d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Swapnil\\Narrowband_DRONE\\Image_compression_code\\Method_7_compress_ai\\compressAIenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Swapnil\\Narrowband_DRONE\\Image_compression_code\\Method_7_compress_ai\\compressAIenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: C:\\Swapnil\\Narrowband_DRONE\\Image_compression_code\\Method_7_compress_ai\\compressAIenv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Swapnil\\Narrowband_DRONE\\Image_compression_code\\Method_7_compress_ai\\compressAIenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: C:\\Swapnil\\Narrowband_DRONE\\Image_compression_code\\Method_7_compress_ai\\compressAIenv\\Lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n"
     ]
    }
   ],
   "source": [
    "#importing\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_msssim import ms_ssim #metric for the image\n",
    "import pandas as pd\n",
    "\n",
    "import pillow_avif #AVIF encoder plugin for the Pillow installed using, pip install pillow-avif-plugin\n",
    "\n",
    "import lpips #already installed with pip install lpips \n",
    "#used for the lpips metric\n",
    "loss_fn_alex=lpips.LPIPS(net=\"alex\")\n",
    "loss_fn_vgg=lpips.LPIPS(net=\"vgg\") \n",
    "\n",
    "#another image reconstruction quality metric\n",
    "from skimage.metrics import structural_similarity as ssim \n",
    "#PSNR metric for image reconstruction\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "\n",
    "from compressai.zoo import bmshj2018_factorized #VAE model for image compression\n",
    "from ipywidgets import interact, widgets \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db9fe67f-9e1e-4c87-93d0-c2703506735c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n"
     ]
    }
   ],
   "source": [
    "#setting the CUDA device\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"   #cpu and cuda in small letters important\n",
    "print(\"Using:\",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4cf1378-066d-419f-b466-661ecb55ce7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 7030531\n"
     ]
    }
   ],
   "source": [
    "#Loading the pretrained model \n",
    "quality=6\n",
    "metric=\"ms-ssim\" #MSE\n",
    "model_name=\"cheng2020-anchor\"#\"bmshj2018_factorized\"\n",
    "net=bmshj2018_factorized(quality=quality,metric=metric,pretrained=True).eval().to(device)\n",
    "print(f\"Parameters: {sum(p.numel() for p in net.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c60f9bd0-9fb3-4f64-8bb8-03b0eff89f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metrics for the image reconstruction \n",
    "    \n",
    "    \n",
    "def crop_image(img_org_path,img_recon_path):\n",
    "    \"\"\"\n",
    "        top left X and Y\n",
    "        bottom right X and Y is the crop box\n",
    "    \"\"\"\n",
    "\n",
    "    img_recon=Image.open(img_recon_path)\n",
    "    img_org=Image.open(img_org_path)\n",
    "\n",
    "    output_width, output_height =img_recon.size\n",
    "    original_width,original_height=img_org.size\n",
    "\n",
    "    #checking the dimensions of the images\n",
    "    if original_width==output_width and \\\n",
    "    original_height==output_height:\n",
    "        return None\n",
    "    else:\n",
    "        crop_box = (0, 0, output_width, original_height)\n",
    "        cropped_image = img_recon.crop(crop_box)\n",
    "        #print(cropped_image.size)\n",
    "        return cropped_image\n",
    "\n",
    "def calculate_ssim(img_org,img_recon):\n",
    "    \"\"\"\n",
    "    img_org and img_recon are the path of the \n",
    "    original image and the reconstructed image\n",
    "    \"\"\"\n",
    "    if crop_image(img_org,img_recon) is None:\n",
    "        img_recon=np.asarray(Image.open(img_recon))\n",
    "        img_org=np.asarray(Image.open(img_org))\n",
    "    else:\n",
    "        img_recon=np.asarray(crop_image(img_org,img_recon))\n",
    "        img_org=np.asarray(Image.open(img_org))\n",
    "        \n",
    "        \n",
    "\n",
    "    ssim_value=ssim(img_org,img_recon,win_size=3) #for 3 channel\n",
    "    #print(\"SSIM:\",ssim_value)\n",
    "    return ssim_value\n",
    "\n",
    "def calculate_psnr(img_org,img_recon):\n",
    "\n",
    "    \"\"\"\n",
    "    img_org and img_recon are the path of the \n",
    "    original image and the reconstructed image\n",
    "    \"\"\"\n",
    "    \n",
    "    if crop_image(img_org,img_recon) is None:\n",
    "        img_recon=np.asarray(Image.open(img_recon))\n",
    "        img_org=np.asarray(Image.open(img_org))\n",
    "    else:\n",
    "        img_recon=np.asarray(crop_image(img_org,img_recon))\n",
    "        img_org=np.asarray(Image.open(img_org))\n",
    "\n",
    "    psnr_value=psnr(img_org,img_recon)\n",
    "    #print(\"PSNR:\",psnr_value)\n",
    "    return psnr_value\n",
    "\n",
    "\n",
    "def calculate_lpips(img_org,img_recon):\n",
    "    \"\"\"\n",
    "    img_org and img_recon are the path of the \n",
    "    original image and the reconstructed image\n",
    "    \"\"\"\n",
    "    if crop_image(img_org,img_recon) is None:\n",
    "        img_recon=np.asarray(Image.open(img_recon))\n",
    "        img_org=np.asarray(Image.open(img_org))\n",
    "    else:\n",
    "        img_recon=np.asarray(crop_image(img_org,img_recon))\n",
    "        img_org=np.asarray(Image.open(img_org))\n",
    "    \n",
    "    transform=transforms.ToTensor() #convert the image to Tensor \n",
    "    #print(img_org.shape,img_recon.shape)\n",
    "    img_org=transform(img_org)\n",
    "    img_recon=transform(img_recon)\n",
    "    \n",
    "    lpips=loss_fn_alex.forward(img_org,img_recon)\n",
    "    lpips=lpips.detach().numpy() #to cpu then \n",
    "    #print(\"LPIPS:\",lpips) \n",
    "    return lpips[0][0][0][0]\n",
    "    \n",
    "\n",
    "def compression_ratio(img_1_path,img_2_path):\n",
    "    \"\"\"\n",
    "        Assuming the first image is uncompressed version\n",
    "    \"\"\"\n",
    "    print(img_1_path)\n",
    "    try:\n",
    "        size_1=os.path.getsize(img_1_path) #getting the size of the image on the disc\n",
    "    except:\n",
    "        print(\"img_1 path not valid\")\n",
    "\n",
    "    try:\n",
    "        size_2=os.path.getsize(img_2_path) #getting the size of the image on the disc\n",
    "    except:\n",
    "        print(\"img_2 path not valid\")\n",
    "\n",
    "    ratio=size_1/size_2\n",
    "    #print(\"Compression ratio is:\",ratio)\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25dbfc29-e17d-48cc-89ab-fa07326a37c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the metrics for the batch\n",
    "def batch_metrics(org_path,recon_path):\n",
    "    \"\"\"\n",
    "    Input: Path of the original dataset\n",
    "    Output: Dataframe of reconstruction quality\n",
    "            metrics and compression ratio for each\n",
    "            image in dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    images,path=file_handling(org_path)\n",
    "    images_r,path_r=file_handling(recon_path)\n",
    "        \n",
    "\n",
    "    #initialize the metrics\n",
    "    ssim=[]\n",
    "    psnr=[]\n",
    "    lpips=[]\n",
    "    cr=[]\n",
    "    org_size=[]\n",
    "    recon_size=[]\n",
    "\n",
    "    for img_o,img_r in zip(images,images_r):\n",
    "\n",
    "        print(\"Doing for:\",img_o)\n",
    "\n",
    "        img_o=os.path.join(path,img_o)\n",
    "        img_r=os.path.join(path_r,img_r)\n",
    "        \n",
    "        ratio=compression_ratio(img_o,img_r)\n",
    "        cr.append(round(ratio,2))\n",
    "        \n",
    "        lpips_=calculate_lpips(img_o,img_r)\n",
    "        lpips.append(lpips_)\n",
    "    \n",
    "    \n",
    "        ssim_=calculate_ssim(img_o,img_r)\n",
    "        ssim.append(ssim_)\n",
    "\n",
    "        \n",
    "        psnr_=calculate_psnr(img_o,img_r)\n",
    "        psnr.append(psnr_)\n",
    "\n",
    "        #sizes are in KB\n",
    "        org_size.append(os.path.getsize(os.path.join(path,img_o))/1000)\n",
    "        recon_size.append(os.path.getsize(os.path.join(path_r,img_r))/1000)\n",
    "        \n",
    "\n",
    "    metrics={\"SSIM\":ssim,\n",
    "             \"PSNR\":psnr,\n",
    "             \"LPIPS\":lpips,\n",
    "             \"CR\":cr,\n",
    "            \"ORG_SIZE\":org_size,\n",
    "            \"RECON_SIZE\":recon_size}\n",
    "\n",
    "    #make a dataframe from the metrics for exporting\n",
    "    df=pd.DataFrame(metrics)\n",
    "    return df\n",
    "\n",
    "def average_batch_metrics(df):\n",
    "    \"\"\"\n",
    "    Gives the average of each of the metric \n",
    "    for the whole batch\n",
    "    \"\"\"\n",
    "    cols=df.columns\n",
    "    avg_metric={}\n",
    "\n",
    "    #number of images\n",
    "    avg_metric[\"Dataset_size\"]=len(df[cols[0]])\n",
    "    for metric in cols:\n",
    "        metric_array=np.array(df[metric]) #convert the each column of df to np array\n",
    "        avg_metric[metric]=round(np.average(metric_array),2)\n",
    "\n",
    "    print(\"average_metrics:\",avg_metric)\n",
    "    return avg_metric\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c471f8be-17e3-4057-bb37-1ad8f32f2e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to the images dataset\n",
    "#path=r\"C:\\Swapnil\\Narrowband_DRONE\\Drone_artificial_dataset_experiment\\images\"\n",
    "\n",
    "def file_handling(path):\n",
    "    \"\"\"\n",
    "    path to the images dataset\n",
    "    \"\"\"\n",
    "    #org_dir=os.getcwd()\n",
    "\n",
    "    #changing to the dataset dir\n",
    "    try:\n",
    "        os.chdir(path)\n",
    "    except:\n",
    "        print(\"Path not valid\")\n",
    "        return\n",
    "\n",
    "    files=os.listdir(path) #files in the directory\n",
    "    images=[] #getting only the jpg and png images\n",
    "    for file in files:\n",
    "        if file.lower().endswith(\".jpeg\") or file.lower().endswith(\".jpg\") \\\n",
    "        or file.lower().endswith(\".png\") or file.lower().endswith(\".webp\") \\\n",
    "        or file.lower().endswith(\".avif\"):\n",
    "            images.append(file)\n",
    "\n",
    "    print(\"Images in dataset:\",len(images))\n",
    "    if len(images)!=0:\n",
    "        print(\"First filename:\",images[0])\n",
    "    else:\n",
    "        print(\"No images in the path or different format\")\n",
    "    \n",
    "    #go to the original directory back\n",
    "    #os.chdir(org_dir)\n",
    "    \n",
    "    return images, path\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2697a561-3ce7-4627-b71f-af68cbbc542a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "134a3dc5-5637-40d9-aea1-a51c8fdb1413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images in dataset: 100\n",
      "First filename: 0300.jpg\n",
      "Images in dataset: 6\n",
      "First filename: 0300.webp\n",
      "Doing for: 0300.jpg\n",
      "C:\\Swapnil\\Narrowband_DRONE\\Drone_artificial_dataset_experiment\\images\\0300.jpg\n",
      "Doing for: 0301.jpg\n",
      "C:\\Swapnil\\Narrowband_DRONE\\Drone_artificial_dataset_experiment\\images\\0301.jpg\n",
      "Doing for: 0302.jpg\n",
      "C:\\Swapnil\\Narrowband_DRONE\\Drone_artificial_dataset_experiment\\images\\0302.jpg\n",
      "Doing for: 0303.jpg\n",
      "C:\\Swapnil\\Narrowband_DRONE\\Drone_artificial_dataset_experiment\\images\\0303.jpg\n",
      "Doing for: 0304.jpg\n",
      "C:\\Swapnil\\Narrowband_DRONE\\Drone_artificial_dataset_experiment\\images\\0304.jpg\n",
      "Doing for: 0305.jpg\n",
      "C:\\Swapnil\\Narrowband_DRONE\\Drone_artificial_dataset_experiment\\images\\0305.jpg\n",
      "average_metrics: {'Dataset_size': 6, 'SSIM': 0.93, 'PSNR': 28.48, 'LPIPS': 0.12, 'CR': 3.32, 'ORG_SIZE': 887.08, 'RECON_SIZE': 268.21}\n"
     ]
    }
   ],
   "source": [
    "#testing only\n",
    "dataset_path=path\n",
    "export_path=os.path.join(dataset_path,\"Data_webp\")\n",
    "df=batch_metrics(dataset_path,export_path) #calculate the metrics of the batch\n",
    "avg_metrics=average_batch_metrics(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "537c433e-91b1-43f3-bde5-7517ba9d9964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_dataset(model,images,dataset_path,img_type='RGB',export_format='webp'):\n",
    "    '''\n",
    "    img_type argument expects 'RGB' for color image and 'L' for grayscale\n",
    "    USES save_image method which does not have controls\n",
    "    '''\n",
    "    img_details={} #dictionary to store the details of the images\n",
    "\n",
    "    if export_format=='webp':\n",
    "        extension='.webp'\n",
    "\n",
    "    #make the export directory\n",
    "    try:\n",
    "        directory_name=\"Data_\"+export_format\n",
    "        os.mkdir(directory_name)\n",
    "    except:\n",
    "        print(\"Directory already exists\")\n",
    "    print(dataset_path)\n",
    "    export_path=os.path.join(dataset_path,directory_name)\n",
    "    \n",
    "    for img in images:\n",
    "        print(\"Doing for:\",img)\n",
    "        img_path=os.path.join(dataset_path,img)\n",
    "        #print(\"img_path:\",img_path)\n",
    "        #open the img\n",
    "        image=Image.open(img_path).convert(img_type) #opening the image as RGB\n",
    "        x=transforms.ToTensor()(image).unsqueeze(0).to(device) #convert to PyTorch tensor and send to device\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            out_net=net.forward(x)\n",
    "        out_net['x_hat'].clamp(0,1)\n",
    "        obj=out_net['x_hat']\n",
    "\n",
    "        #save the output of the network as image\n",
    "        #saving in the tensor form takes lot of space that is why\n",
    "        #directly exporting the tensor output as image (94 MB vs 1MB)\n",
    "        #print(img)\n",
    "        img=img.split('.')[0] + extension #getting the image name and changing the extension\n",
    "        img=os.path.join(export_path,img)\n",
    "        save_image(obj,img)\n",
    "    return export_path\n",
    "\n",
    "    \n",
    "    #df=batch_metrics(dataset_path,export_path) #calculate the metrics of the batch\n",
    "    #avg_metrics=average_batch_metrics(df)\n",
    "\n",
    "#path=r\"C:\\Swapnil\\Narrowband_DRONE\\Drone_atsugi_dataset_exp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f755227-16e5-4b14-b6ab-0115edfd7383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_FHD(image,path):\n",
    "    image_path=os.path.join(path,image)\n",
    "    print(\"path:\",image_path)\n",
    "    output_path=os.path.join(path,image.split('.')[0]+\"_r.png\")\n",
    "    print(\"output:\",output_path)\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "          # Resize the image to maintain aspect ratio\n",
    "            print(\"Image opened\")\n",
    "            img.thumbnail((1920, 1080), Image.LANCZOS)\n",
    "            img.save(output_path)\n",
    "        print(f\"Image resized to Full HD and saved to: {output_path}\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error opening or saving image: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e639345f-d4c0-4ff8-8aeb-a18e912acbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images in dataset: 3\n",
      "First filename: 000000_left_r.png\n",
      "Images: ['000000_left_r.png', '000020_left_r.png', '000074_left_r.png']\n",
      "path: C:\\Swapnil\\Narrowband_DRONE\\Drone_dataset_small_1\\exp\\000000_left_r.png\n",
      "output: C:\\Swapnil\\Narrowband_DRONE\\Drone_dataset_small_1\\exp\\000000_left_r_r.png\n",
      "Image opened\n",
      "Image resized to Full HD and saved to: C:\\Swapnil\\Narrowband_DRONE\\Drone_dataset_small_1\\exp\\000000_left_r_r.png\n",
      "path: C:\\Swapnil\\Narrowband_DRONE\\Drone_dataset_small_1\\exp\\000020_left_r.png\n",
      "output: C:\\Swapnil\\Narrowband_DRONE\\Drone_dataset_small_1\\exp\\000020_left_r_r.png\n",
      "Image opened\n",
      "Image resized to Full HD and saved to: C:\\Swapnil\\Narrowband_DRONE\\Drone_dataset_small_1\\exp\\000020_left_r_r.png\n",
      "path: C:\\Swapnil\\Narrowband_DRONE\\Drone_dataset_small_1\\exp\\000074_left_r.png\n",
      "output: C:\\Swapnil\\Narrowband_DRONE\\Drone_dataset_small_1\\exp\\000074_left_r_r.png\n",
      "Image opened\n",
      "Image resized to Full HD and saved to: C:\\Swapnil\\Narrowband_DRONE\\Drone_dataset_small_1\\exp\\000074_left_r_r.png\n"
     ]
    }
   ],
   "source": [
    "path=r'C:\\Swapnil\\Narrowband_DRONE\\Drone_dataset_small_1\\exp'\n",
    "images,path=file_handling(path)\n",
    "print(\"Images:\",images)\n",
    "for image in images:\n",
    "    resize_FHD(image,path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34154135-b9e6-48e5-9f59-f12c16ee54dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images in dataset: 3\n",
      "First filename: 000000_left.png\n",
      "Directory already exists\n",
      "C:\\Swapnil\\Narrowband_DRONE\\Drone_dataset_small_1\\exp\n",
      "Doing for: 000000_left.png\n",
      "Doing for: 000020_left.png\n",
      "Doing for: 000074_left.png\n",
      "Average time per image: 10.31\n"
     ]
    }
   ],
   "source": [
    "path=r'C:\\Swapnil\\Narrowband_DRONE\\Drone_dataset_small_1\\exp'\n",
    "import time\n",
    "images,path=file_handling(path)\n",
    "start=time.time()\n",
    "compress_dataset(net,images,path)\n",
    "end=time.time()\n",
    "print(\"Average time per image:\",round(end-start,2)/len(images))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3550cc04-d178-4513-b595-e27c0bc6f77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images in dataset: 3\n",
      "First filename: 000000_left.webp\n",
      "Doing for: 000000_left.webp\n",
      "Doing for: 000020_left.webp\n",
      "Doing for: 000074_left.webp\n"
     ]
    }
   ],
   "source": [
    "#Stage 2 compression \n",
    "#OverWriting the stage 1 compression results from stage 2\n",
    "def compress_stage_2(img_org_path,img_recon_path,export_format=\"avif\",quality=5):\n",
    "    \"\"\" \n",
    "    Input: Path to already compressed images from method save_image\n",
    "    Output: OverWrite the existing images\n",
    "    Use Pillow here to export the images\n",
    "\n",
    "    export_format=\"avif\",\"webp\"\n",
    "    quality=0 to 100\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    img_details={} #dictionary to store the details of the images\n",
    "\n",
    "    if export_format=='webp':\n",
    "        extension='.webp'\n",
    "    else:\n",
    "        extension='.avif'\n",
    "\n",
    "    #make the export directory\n",
    "    #try:\n",
    "    #    directory_name=\"Data_\"+export_format\n",
    "    #    os.mkdir(directory_name)\n",
    "    #except:\n",
    "    #    print(\"Directory already exists\")\n",
    "    #print(dataset_path)\n",
    "    \n",
    "    #export_path=os.path.join(dataset_path,directory_name)\n",
    "\n",
    "    images,path=file_handling(img_recon_path)\n",
    "    \n",
    "    for img in images:\n",
    "        print(\"Doing for:\",img)\n",
    "        img_path=os.path.join(path,img)\n",
    "        image_name=img.split('.')[0] + \"_c\"+ extension #getting the image name and changing the extension\n",
    "        with Image.open(img_path) as image:\n",
    "            image.save(image_name,format=export_format.upper(),quality=quality)\n",
    "        #remove previous file\n",
    "        os.remove(img)\n",
    "img_recon_path=r\"C:\\Swapnil\\Narrowband_DRONE\\Drone_dataset_small_1\\exp\\Data_webp\"\n",
    "org_path=path\n",
    "compress_stage_2(org_path,img_recon_path,export_format=\"webp\")\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af610b79-1a69-498b-b2e6-44a60a7ff430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images in dataset: 3\n",
      "First filename: 000000_left.png\n",
      "Images in dataset: 3\n",
      "First filename: 000000_left_c.webp\n",
      "Doing for: 000000_left.png\n",
      "C:\\Swapnil\\Narrowband_DRONE\\Drone_dataset_small_1\\exp\\000000_left.png\n",
      "Doing for: 000020_left.png\n",
      "C:\\Swapnil\\Narrowband_DRONE\\Drone_dataset_small_1\\exp\\000020_left.png\n",
      "Doing for: 000074_left.png\n",
      "C:\\Swapnil\\Narrowband_DRONE\\Drone_dataset_small_1\\exp\\000074_left.png\n",
      "average_metrics: {'Dataset_size': 3, 'SSIM': 0.87, 'PSNR': 29.06, 'LPIPS': 0.12, 'CR': 56.46, 'ORG_SIZE': 10233.49, 'RECON_SIZE': 189.97}\n"
     ]
    }
   ],
   "source": [
    "#calculate the metrics with respect to the original and the second stage image compression \n",
    "dataset_path=r'C:\\Swapnil\\Narrowband_DRONE\\Drone_dataset_small_1\\exp' #org_path\n",
    "export_path=img_recon_path\n",
    "df=batch_metrics(dataset_path,export_path) #calculate the metrics of the batch\n",
    "avg_metrics=average_batch_metrics(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecb668a-fd01-4416-804c-e73e81b2621d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
