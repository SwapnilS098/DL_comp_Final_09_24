{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cef5153-6fda-4011-91b4-0dd6ea6f201d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import io\n",
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ebf2023-a637-4d9e-b3ee-1a9ed5d5b96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from compressai.zoo import bmshj2018_factorized_relu\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#device=\"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ca8e57f-cce7-435a-afaf-8d6a32f346be",
   "metadata": {},
   "outputs": [],
   "source": [
    "quality=8\n",
    "metric=\"mse\" #\"ms-ssim\"\n",
    "model_name=\"bmshj2018_factorized_relu\"\n",
    "net = bmshj2018_factorized_relu(quality=quality,metric=metric, pretrained=True).eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a7c99c4-85cc-4c82-9a19-5a70ba435801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FactorizedPrior(\n",
      "  (entropy_bottleneck): EntropyBottleneck(\n",
      "    (likelihood_lower_bound): LowerBound()\n",
      "    (matrices): ParameterList(\n",
      "        (0): Parameter containing: [torch.float32 of size 320x3x1 (cuda:0)]\n",
      "        (1): Parameter containing: [torch.float32 of size 320x3x3 (cuda:0)]\n",
      "        (2): Parameter containing: [torch.float32 of size 320x3x3 (cuda:0)]\n",
      "        (3): Parameter containing: [torch.float32 of size 320x3x3 (cuda:0)]\n",
      "        (4): Parameter containing: [torch.float32 of size 320x1x3 (cuda:0)]\n",
      "    )\n",
      "    (biases): ParameterList(\n",
      "        (0): Parameter containing: [torch.float32 of size 320x3x1 (cuda:0)]\n",
      "        (1): Parameter containing: [torch.float32 of size 320x3x1 (cuda:0)]\n",
      "        (2): Parameter containing: [torch.float32 of size 320x3x1 (cuda:0)]\n",
      "        (3): Parameter containing: [torch.float32 of size 320x3x1 (cuda:0)]\n",
      "        (4): Parameter containing: [torch.float32 of size 320x1x1 (cuda:0)]\n",
      "    )\n",
      "    (factors): ParameterList(\n",
      "        (0): Parameter containing: [torch.float32 of size 320x3x1 (cuda:0)]\n",
      "        (1): Parameter containing: [torch.float32 of size 320x3x1 (cuda:0)]\n",
      "        (2): Parameter containing: [torch.float32 of size 320x3x1 (cuda:0)]\n",
      "        (3): Parameter containing: [torch.float32 of size 320x3x1 (cuda:0)]\n",
      "    )\n",
      "  )\n",
      "  (g_a): Sequential(\n",
      "    (0): Conv2d(3, 192, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "    (1): GDN(\n",
      "      (beta_reparam): NonNegativeParametrizer(\n",
      "        (lower_bound): LowerBound()\n",
      "      )\n",
      "      (gamma_reparam): NonNegativeParametrizer(\n",
      "        (lower_bound): LowerBound()\n",
      "      )\n",
      "    )\n",
      "    (2): Conv2d(192, 192, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "    (3): GDN(\n",
      "      (beta_reparam): NonNegativeParametrizer(\n",
      "        (lower_bound): LowerBound()\n",
      "      )\n",
      "      (gamma_reparam): NonNegativeParametrizer(\n",
      "        (lower_bound): LowerBound()\n",
      "      )\n",
      "    )\n",
      "    (4): Conv2d(192, 192, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "    (5): GDN(\n",
      "      (beta_reparam): NonNegativeParametrizer(\n",
      "        (lower_bound): LowerBound()\n",
      "      )\n",
      "      (gamma_reparam): NonNegativeParametrizer(\n",
      "        (lower_bound): LowerBound()\n",
      "      )\n",
      "    )\n",
      "    (6): Conv2d(192, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  )\n",
      "  (g_s): Sequential(\n",
      "    (0): ConvTranspose2d(320, 192, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "    (1): GDN(\n",
      "      (beta_reparam): NonNegativeParametrizer(\n",
      "        (lower_bound): LowerBound()\n",
      "      )\n",
      "      (gamma_reparam): NonNegativeParametrizer(\n",
      "        (lower_bound): LowerBound()\n",
      "      )\n",
      "    )\n",
      "    (2): ConvTranspose2d(192, 192, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "    (3): GDN(\n",
      "      (beta_reparam): NonNegativeParametrizer(\n",
      "        (lower_bound): LowerBound()\n",
      "      )\n",
      "      (gamma_reparam): NonNegativeParametrizer(\n",
      "        (lower_bound): LowerBound()\n",
      "      )\n",
      "    )\n",
      "    (4): ConvTranspose2d(192, 192, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "    (5): GDN(\n",
      "      (beta_reparam): NonNegativeParametrizer(\n",
      "        (lower_bound): LowerBound()\n",
      "      )\n",
      "      (gamma_reparam): NonNegativeParametrizer(\n",
      "        (lower_bound): LowerBound()\n",
      "      )\n",
      "    )\n",
      "    (6): ConvTranspose2d(192, 3, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3027d39-f7ba-45fe-aa6a-871d26854d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Swapnil\\Narrowband_DRONE\\Image_compression_code\\Method_7_compress_ai\\compressai\\compressai\\entropy_models\\entropy_models.py:476: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  torch.tensor([1, 0], dtype=torch.long, device=x.device),\n",
      "C:\\Swapnil\\Narrowband_DRONE\\Image_compression_code\\Method_7_compress_ai\\compressai\\compressai\\entropy_models\\entropy_models.py:488: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  x = x.permute(*perm).contiguous()\n",
      "C:\\Swapnil\\Narrowband_DRONE\\Image_compression_code\\Method_7_compress_ai\\compressai\\compressai\\entropy_models\\entropy_models.py:488: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  x = x.permute(*perm).contiguous()\n",
      "C:\\Swapnil\\Narrowband_DRONE\\Image_compression_code\\Method_7_compress_ai\\compressai\\compressai\\entropy_models\\entropy_models.py:509: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  outputs = outputs.permute(*inv_perm).contiguous()\n",
      "C:\\Swapnil\\Narrowband_DRONE\\Image_compression_code\\Method_7_compress_ai\\compressai\\compressai\\entropy_models\\entropy_models.py:509: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  outputs = outputs.permute(*inv_perm).contiguous()\n",
      "C:\\Swapnil\\Narrowband_DRONE\\Image_compression_code\\Method_7_compress_ai\\compressai\\compressai\\entropy_models\\entropy_models.py:512: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  likelihood = likelihood.permute(*inv_perm).contiguous()\n",
      "C:\\Swapnil\\Narrowband_DRONE\\Image_compression_code\\Method_7_compress_ai\\compressai\\compressai\\entropy_models\\entropy_models.py:512: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  likelihood = likelihood.permute(*inv_perm).contiguous()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is exported\n",
      "size of x: 6061440\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import onnx\n",
    "from compressai.zoo import models\n",
    "import onnxruntime\n",
    "\n",
    "net = models[\"bmshj2018-factorized\"](quality=6, metric=\"mse\", pretrained=True)\n",
    "# net = cheng2020_anchor(quality=5, pretrained=True).to(device)\n",
    "\n",
    "# Some dummy input\n",
    "x = torch.randn(1, 3, 1232, 1640, requires_grad=True)\n",
    "\n",
    "export_path=r\"C:\\Swapnil\\Narrowband_DRONE\\Image_compression_code\\Method_7_compress_ai\\compressai\\examples\\bmshj2018_factorized_models\"\n",
    "export_path=os.path.join(export_path,\"bmshj_model_GPU.onnx\")\n",
    "# Export the model\n",
    "torch.onnx.export(net,                       # model being run\n",
    "                  x,                         # model input (or a tuple for multiple inputs)\n",
    "                  export_path,              # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=14,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['input'],   # the model's input names\n",
    "                  output_names = ['output'], # the model's output names\n",
    "                  dynamic_axes={'input': {0 : 'batch_size'},    # variable length axes\n",
    "                                'output': {0 : 'batch_size'}}\n",
    "                 )\n",
    "print(\"Model is exported\")\n",
    "\n",
    "onnx_model = onnx.load(export_path)\n",
    "onnx_model_graph = onnx_model.graph\n",
    "onnx_session = onnxruntime.InferenceSession(onnx_model.SerializeToString())\n",
    "# onnx_session = onnxruntime.InferenceSession(\"cheng2020.onnx\")\n",
    "\n",
    "input_shape = (1, 3, 1232, 1640)\n",
    "x = torch.randn(input_shape).numpy()\n",
    "print(\"size of x:\",x.size)\n",
    "\n",
    "input_names = [\"input\"]\n",
    "output_names = [\"output\"]\n",
    "\n",
    "onnx_output = onnx_session.run(output_names, {input_names[0]: x})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0daaf8e-df90-4a2f-aa05-fa4b998e132e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using execution provider(s) :['CPUExecutionProvider']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: ['CUDAExecutionProvider']\n",
      "Time in seconds for the inference is: 4.96 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ONNX Runtime session created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed image saved at: C:\\Swapnil\\Narrowband_DRONE\\Image_compression_code\\FLOPS\\image_py_onnx_export_GPU.png\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import onnx\n",
    "import numpy as np\n",
    "import onnxruntime\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"\n",
    "    Preprocess the input image to the format expected by the model.\n",
    "    \"\"\"\n",
    "    # Load the image using PIL\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    image=image.resize((1640,1232))\n",
    "    \n",
    "    # Convert to numpy array and normalize the image (0 to 1)\n",
    "    image = np.array(image) / 255.0\n",
    "    \n",
    "    # Convert to CHW format (Channels x Height x Width)\n",
    "    image = np.transpose(image, (2, 0, 1))\n",
    "    \n",
    "    # Add a batch dimension (1, Channels, Height, Width)\n",
    "    image = np.expand_dims(image, axis=0).astype(np.float32)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def save_compressed_image(output, save_path):\n",
    "    \"\"\"\n",
    "    Save the compressed output image from the model inference.\n",
    "    \"\"\"\n",
    "    # Post-process the output (if needed)\n",
    "    output = output.squeeze(0)  # Remove batch dimension\n",
    "    \n",
    "    # Convert to HWC format (Height x Width x Channels)\n",
    "    output = np.transpose(output, (1, 2, 0))\n",
    "    \n",
    "    # Clip values to valid range (0, 1) and convert to 8-bit (0-255)\n",
    "    output = np.clip(output, 0, 1) * 255.0\n",
    "    output = output.astype(np.uint8)\n",
    "    \n",
    "    # Convert to image format and save\n",
    "    output_image = Image.fromarray(output)\n",
    "    output_image.save(save_path)\n",
    "\n",
    "def run_inference(image_path, model_path, save_path):\n",
    "    \"\"\"\n",
    "    Run inference on an input image using the exported ONNX model.\n",
    "    \"\"\"\n",
    "    # Load the image and preprocess it\n",
    "    input_image = preprocess_image(image_path)\n",
    "    \n",
    "    # Load the ONNX model\n",
    "\n",
    "    session_options = onnxruntime.SessionOptions()\n",
    "    session_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "    #session_options.log_severity_level = onnxruntime.logging.LoggingLevel.WARNING\n",
    "\n",
    "    #Check if CUDA is available\n",
    "    providers = ['CUDAExecutionProvider'] if 'CUDAExecutionProvider' in onnxruntime.get_available_providers() else ['CPUExecutionProvider']\n",
    "    print(\"Using:\",providers)\n",
    "    onnx_session = onnxruntime.InferenceSession(model_path,sess_options=session_options, providers=providers)\n",
    "    #onnx_session = onnxruntime.InferenceSession(model_path)\n",
    "\n",
    "    logging.info(f\"Using execution provider(s) :{onnx_session.get_providers()}\")\n",
    "    \n",
    "    # Run inference\n",
    "    input_names = [\"input\"]\n",
    "    output_names = [\"output\"]\n",
    "    start=time.time()\n",
    "    onnx_output = onnx_session.run(output_names, {input_names[0]: input_image})[0]\n",
    "    end=time.time()\n",
    "\n",
    "    print(\"Time in seconds for the inference is:\",round(end-start,2),\"seconds\")\n",
    "\n",
    "    \n",
    "    # Save the output image\n",
    "    save_compressed_image(onnx_output, save_path)\n",
    "    print(f\"Compressed image saved at: {save_path}\")\n",
    "import logging\n",
    "# Configure logging\n",
    "#logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
    "\n",
    "image_path=r\"C:\\Swapnil\\Narrowband_DRONE\\Image_compression_code\\FLOPS\\image.png\"\n",
    "model_path=r\"C:\\Swapnil\\Narrowband_DRONE\\Image_compression_code\\Method_7_compress_ai\\compressai\\examples\\bmshj2018_factorized_models\\bmshj_model_GPU.onnx\"\n",
    "save_path=r\"C:\\Swapnil\\Narrowband_DRONE\\Image_compression_code\\FLOPS\\image_py_onnx_export_GPU.png\"\n",
    "\n",
    "run_inference(image_path,model_path,save_path)\n",
    "# Log a message\n",
    "logging.info(\"ONNX Runtime session created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a8921e6-118d-47c6-b703-d20e60ffd04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "print(onnxruntime.get_available_providers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d9a2d6d0-6a35-4812-b273-19ca4cf0f2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (1, 3, 1232, 1648) 6091008\n"
     ]
    }
   ],
   "source": [
    "print(type(onnx_output),onnx_output.shape,onnx_output.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0f439231-7cf2-4daf-9db7-8fb142c9a0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporting the BMSHJ model to onnx\n",
    "#torch_model=net\n",
    "#torch_input=torch.randn(1,1,1280,720)\n",
    "\n",
    "#options and method to run the inference from the ONNX model using the GPU\n",
    "\n",
    "#session_options = onnxruntime.SessionOptions()\n",
    "#session_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "#session = onnxruntime.InferenceSession(export_path,sess_options=session_options, providers=['CUDAExecutionProvider'])\n",
    "#onnx_program=torch.onnx.dynamo_export(torch_model,torch_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2fdccd2b-2f34-4162-9d38-18eb70fbc4a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "OnnxExporterError",
     "evalue": "Failed to export the model to ONNX. Generating SARIF report at 'report_dynamo_export.sarif'. SARIF is a standard format for the output of static analysis tools. SARIF logs can be loaded in VS Code SARIF viewer extension, or SARIF web viewer (https://microsoft.github.io/sarif-web-component/). Please report a bug on PyTorch Github: https://github.com/pytorch/pytorch/issues",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mC:\\Swapnil\\Narrowband_DRONE\\Image_compression_code\\Method_7_compress_ai\\compressAIenv\\Lib\\site-packages\\torch\\onnx\\_internal\\exporter.py:1433\u001b[0m, in \u001b[0;36mdynamo_export\u001b[1;34m(model, export_options, *model_args, **model_kwargs)\u001b[0m\n\u001b[0;32m   1427\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1428\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mExporter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1429\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolved_export_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 1433\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1434\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\Swapnil\\Narrowband_DRONE\\Image_compression_code\\Method_7_compress_ai\\compressAIenv\\Lib\\site-packages\\torch\\onnx\\_internal\\exporter.py:1175\u001b[0m, in \u001b[0;36mExporter.export\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mdiagnostic_context:\n\u001b[1;32m-> 1175\u001b[0m     graph_module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfx_tracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_fx\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1176\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\n\u001b[0;32m   1177\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1179\u001b[0m     \u001b[38;5;66;03m# TODO: Defer `import onnxscript` out of `import torch` path\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/103764\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Swapnil\\Narrowband_DRONE\\Image_compression_code\\Method_7_compress_ai\\compressAIenv\\Lib\\site-packages\\torch\\onnx\\_internal\\fx\\dynamo_graph_extractor.py:213\u001b[0m, in \u001b[0;36mDynamoExport.generate_fx\u001b[1;34m(self, options, model, model_args, model_kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fake_mode:  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m--> 213\u001b[0m     graph_module, graph_guard \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrapped_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtracing_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfx_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m graph_guard  \u001b[38;5;66;03m# Unused\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Swapnil\\Narrowband_DRONE\\Image_compression_code\\Method_7_compress_ai\\compressAIenv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:1246\u001b[0m, in \u001b[0;36mexport.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1245\u001b[0m assume_static_by_default \u001b[38;5;241m=\u001b[39m _assume_static_by_default\n\u001b[1;32m-> 1246\u001b[0m \u001b[43mcheck_if_dynamo_supported\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1247\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_once(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch._dynamo.export\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Swapnil\\Narrowband_DRONE\\Image_compression_code\\Method_7_compress_ai\\compressAIenv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:702\u001b[0m, in \u001b[0;36mcheck_if_dynamo_supported\u001b[1;34m()\u001b[0m\n\u001b[0;32m    701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m12\u001b[39m):\n\u001b[1;32m--> 702\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython 3.12+ not yet supported for torch.compile\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Python 3.12+ not yet supported for torch.compile",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOnnxExporterError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m torch_model\u001b[38;5;241m=\u001b[39mMyModel() \u001b[38;5;66;03m#PyTorch model\u001b[39;00m\n\u001b[0;32m     30\u001b[0m torch_input\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m32\u001b[39m) \u001b[38;5;66;03m#input for the torch model\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m onnx_program\u001b[38;5;241m=\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdynamo_export\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtorch_input\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#export the pytorch model to onnx\u001b[39;00m\n\u001b[0;32m     33\u001b[0m onnx_program\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmy_image_classifier.onnx\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m#save the model to disc\u001b[39;00m\n\u001b[0;32m     36\u001b[0m onnx_model \u001b[38;5;241m=\u001b[39m onnx\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_image_classifier.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m#loading the onnx model\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Swapnil\\Narrowband_DRONE\\Image_compression_code\\Method_7_compress_ai\\compressAIenv\\Lib\\site-packages\\torch\\onnx\\_internal\\exporter.py:1444\u001b[0m, in \u001b[0;36mdynamo_export\u001b[1;34m(model, export_options, *model_args, **model_kwargs)\u001b[0m\n\u001b[0;32m   1436\u001b[0m resolved_export_options\u001b[38;5;241m.\u001b[39mdiagnostic_context\u001b[38;5;241m.\u001b[39mdump(sarif_report_path)\n\u001b[0;32m   1437\u001b[0m message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1438\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to export the model to ONNX. Generating SARIF report at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msarif_report_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSARIF is a standard format for the output of static analysis tools. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1442\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease report a bug on PyTorch Github: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_PYTORCH_GITHUB_ISSUES_URL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1443\u001b[0m )\n\u001b[1;32m-> 1444\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m OnnxExporterError(\n\u001b[0;32m   1445\u001b[0m     ONNXProgram\u001b[38;5;241m.\u001b[39m_from_failure(e, resolved_export_options\u001b[38;5;241m.\u001b[39mdiagnostic_context),\n\u001b[0;32m   1446\u001b[0m     message,\n\u001b[0;32m   1447\u001b[0m ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mOnnxExporterError\u001b[0m: Failed to export the model to ONNX. Generating SARIF report at 'report_dynamo_export.sarif'. SARIF is a standard format for the output of static analysis tools. SARIF logs can be loaded in VS Code SARIF viewer extension, or SARIF web viewer (https://microsoft.github.io/sarif-web-component/). Please report a bug on PyTorch Github: https://github.com/pytorch/pytorch/issues"
     ]
    }
   ],
   "source": [
    "\n",
    "#CNN model \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "import onnx\n",
    "import onnxruntime\n",
    "torch_model=MyModel() #PyTorch model\n",
    "torch_input=torch.randn(1,1,32,32) #input for the torch model\n",
    "onnx_program=torch.onnx.dynamo_export(torch_model,torch_input) #export the pytorch model to onnx\n",
    "\n",
    "onnx_program.save('my_image_classifier.onnx') #save the model to disc\n",
    "\n",
    "\n",
    "onnx_model = onnx.load(\"my_image_classifier.onnx\") #loading the onnx model\n",
    "onnx.checker.check_model(onnx_model)#check the model\n",
    "\n",
    "\n",
    "onnx_input = onnx_program.adapt_torch_inputs_to_onnx(torch_input)\n",
    "print(f\"Input length: {len(onnx_input)}\")\n",
    "print(f\"Sample input: {onnx_input}\")\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(\"./my_image_classifier.onnx\", providers=['CPUExecutionProvider'])\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "onnxruntime_input = {k.name: to_numpy(v) for k, v in zip(ort_session.get_inputs(), onnx_input)}\n",
    "\n",
    "onnxruntime_outputs = ort_session.run(None, onnxruntime_input)\n",
    "\n",
    "torch_outputs = torch_model(torch_input)\n",
    "torch_outputs = onnx_program.adapt_torch_outputs_to_onnx(torch_outputs)\n",
    "\n",
    "assert len(torch_outputs) == len(onnxruntime_outputs)\n",
    "for torch_output, onnxruntime_output in zip(torch_outputs, onnxruntime_outputs):\n",
    "    torch.testing.assert_close(torch_output, torch.tensor(onnxruntime_output))\n",
    "\n",
    "print(\"PyTorch and ONNX Runtime output matched!\")\n",
    "print(f\"Output length: {len(onnxruntime_outputs)}\")\n",
    "print(f\"Sample output: {onnxruntime_outputs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945b4016-725c-4ac3-8bac-e0291343fa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8820d65-a6da-4a46-b241-a2b47a1adbff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch and ONNX Runtime output matched!\n",
      "Output length: 1\n",
      "Sample output: [array([[ 0.01380568, -0.0715049 , -0.02148017, -0.10276306,  0.00692059,\n",
      "        -0.1345982 ,  0.04789107, -0.02346265, -0.13729787,  0.01007874]],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f0c23b-c7e9-41bf-94e6-cfa5c2e50a58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
